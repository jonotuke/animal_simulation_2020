---
title: "Notes on simulation"
author: "Jono Tuke"
date: "`r format(lubridate::today(), '%a %d %b %Y')`"
output:
  html_document:
    theme: spacelab
    number_sections: yes
    df_print: paged
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  fig.width = 6, 
  fig.asp = 0.618, 
  out.width = "100%",
  fig.align = "center", 
  root.dir = '../'
)
pacman::p_load(tidyverse, tidymodels, targets, gt, rstatix)
dir("R/", full.names = TRUE) %>% walk(source)
dir("../R/", full.names = TRUE) %>% walk(source)
theme_set(theme_bw())
```

# Basic form for simulated data

The basic form, referred to as a `simDB`, is a data frame such that that each row is a second, it has the following columns

-   **time:** the second of interest.
-   **type:** the type of behaviour

The types of behaviour will of two forms:

-   background
-   event / state

So we are interested in the event / state behaviour.

## Notation

We will represent the data by 

$$
x_1, x_2, \ldots, x_n, 
$$ 

where $n = 3600$ in our case and

$$
x_i = 
\begin{cases}
0, &\text{if time-point }i\text{ is background, }\\
1, &\text{if time-point }i\text{ is an event.}
\end{cases}
$$

## Example

Consider the following data frame

```{r}
tar_load(example_simDB)
example_simDB
```

```{r}
plot_simDB(example_simDB)
```

# Percent of time event occurs

The key measure we are interested in is the percent of the time that the event occurs. We stick to proportion as this is nicer mathematically, and then convert to percent for presentation. 

It is defined as

$$
p_{GS} = \frac1n \sum^n_{i = 1}x_i
$$

```{r}
get_prop(example_simDB) %>% scales::percent()
```

# Simulation method

## Response frequency

In these simulations, we have a behaviour that is short (1 second). The behaviour occurs

-   frequent (every 3 seconds),
-   moderate (every 30 seconds), or
-   infrequent (every 300 seconds).

The function `sim_response_freq()` produces a simDB with these properties. It works by splitting the time into blocks of length equal to the frequency, so for moderate frequency, we have blocks of 30 seconds, then one of the seconds in this time are randomly selected to be the event.

So we split the data

$$
x_1, x_2, \ldots, x_n, 
$$

where 

$$
x_i = 
\begin{cases}
0, &\text{if time-point }i\text{ is background, }\\
1, &\text{if time-point }i\text{ is an event.}
\end{cases}
$$

into $K$ blocks

Block 1

$$
x_1, x_2, \ldots, x_{\delta}
$$

Block 2

$$
x_{\delta + 1}, x_{\delta + 2}, \ldots, x_{2\delta}
$$

So we have

Block $i$

$$
x_{(i-1)\delta + 1}, x_{(i-1)\delta + 2}, \ldots, x_{i\delta}
$$ The final block is

$$
x_{(k - 1)\delta + 1}, x_{(k - 1)\delta + 2}, \ldots, x_n, 
$$ where

$$
k = \left\lceil \frac{n}{\delta}\right\rceil.
$$

We then randomly select one of the time points in the block with equal probability, 

$$
x_j, j \in \{x_{(i-1)\delta + 1}, x_{(i-1)\delta + 2}, \ldots, x_{i\delta}\}
$$ 

then we set

$$
x_j = 1, 
$$

and 

$$
x_i = 0, i \in \{x_{(i-1)\delta + 1}, x_{(i-1)\delta + 2}, \ldots, x_{i\delta}\}, i \ne j.
$$

```{r}
sim_response_freq(3) %>% plot_simDB()
sim_response_freq(30) %>% plot_simDB()
sim_response_freq(300) %>% plot_simDB()
```

```{r}
sim_response_freq(3) %>% get_prop()
sim_response_freq(30) %>% get_prop()
sim_response_freq(300) %>% get_prop()
```

### Joint plot

```{r}
sim_response_freq(3) %>% 
  add_column(rate = "high") %>% 
  bind_rows(
    sim_response_freq(30) %>% 
      add_column(rate = "medium")
  ) %>% 
  bind_rows(
    sim_response_freq(300) %>% 
      add_column(rate = "low")
  ) %>% 
  filter(
    type == "event"
  ) %>% 
  mutate(
    rate = factor(rate, levels = c("low", "medium", "high"))
  ) %>% 
  ggplot(
    aes(time, rate) 
  ) + geom_point(shape = "square") +
  labs(y = "Frequency", x = "Time (seconds)") + 
    scale_x_continuous(breaks = seq(0, 3600, 300))
```

```{r}
ggsave("figs/sim_RF.jpg", width = 10, height = 6)
```

## Response duration

In this simulation, the duration is

-   short (3 second),
-   medium (30 seconds) and
-   long (300 seconds).

A single event occurs in each 10 minute interval, and is randomly placed in this interval.

```{r}
sim_response_duration(3) %>% plot_simDB() + geom_vline(xintercept = seq(1, 3600, 600))
sim_response_duration(30) %>% plot_simDB() + geom_vline(xintercept = seq(1, 3600, 600))
sim_response_duration(300) %>% plot_simDB() + geom_vline(xintercept = seq(1, 3600, 600))
```

```{r}
sim_response_duration(3) %>% get_prop()
sim_response_duration(30) %>% get_prop()
sim_response_duration(300) %>% get_prop()
```

### Joint plot

```{r}
sim_response_duration(3) %>% 
  add_column(rate = "short") %>% 
  bind_rows(
    sim_response_duration(30) %>% 
      add_column(rate = "medium")
  ) %>% 
  bind_rows(
    sim_response_duration(300) %>% 
      add_column(rate = "long")
  ) %>% 
  filter(
    type == "event"
  ) %>% 
  mutate(
    rate = factor(rate, levels = c("long", "medium", "short"))
  ) %>% 
  ggplot(
    aes(time, rate) 
  ) + geom_point(shape = "square") +
  labs(y = "Duration", x = "Time (seconds)") + 
    scale_x_continuous(breaks = seq(0, 3600, 300)) + 
  geom_vline(xintercept = seq(1, 3600, 600))
```

```{r}
ggsave("figs/sim_RD.jpg", width = 10, height = 6)
```

# Sampling methods

So we have two sampling methods:

## Pinpoint sampling

In pinpoint sampling, we sample at regular intervals, so for example, if the continuous data is represented by

$$
x_1, x_2, \ldots, x_n, 
$$ where $n = 3600$ in our case and

$$
x_i = 
\begin{cases}
0, &\text{if time-point }i\text{ is background, }\\
1, &\text{if time-point }i\text{ is an event.}
\end{cases}
$$

Then if we have an interval of $\delta$, then the pinpoint sample is

$$
x_1, x_{1+\delta}, x_{1+ 2 \delta}, \ldots, x_{1 + k\delta}, 
$$ where

$$
k = \left\lfloor \frac{n - 1}{\delta}\right\rfloor
$$

The function `pin_point_sampling()` takes an interval and a `simDB` and returns the proportion of the time-points that are an event, i.e.

$$
p_{pp}(\delta) = \frac{x_1 + x_{1+\delta} +  x_{1+ 2 \delta} +  \ldots + x_{1 + k\delta},}{k+1}
$$

```{r}
pin_point_sampling(example_simDB, 3)
pin_point_sampling(example_simDB, 30)
pin_point_sampling(example_simDB, 300)
```

## One-zero sampling

In one-zero sampling, we split the continuous data into contiguous blocks of length $\delta$, i.e.,

Block 1

$$
x_1, x_2, \ldots, x_{\delta}
$$

Block 2

$$
x_{\delta + 1}, x_{\delta + 2}, \ldots, x_{2\delta}
$$

So we have

Block $i$

$$
x_{(i-1)\delta + 1}, x_{(i-1)\delta + 2}, \ldots, x_{i\delta}
$$ The final block is

$$
x_{(k - 1)\delta + 1}, x_{(k - 1)\delta + 2}, \ldots, x_n, 
$$ where

$$
k = \left\lceil \frac{n}{\delta}\right\rceil.
$$

Note that this final block may not be the same length as the previous blocks.

If we let the result for Block $i$ be denoted by $y_i$, then we have

$$
y_i = \begin{cases}
0, &\text{if all }x_{(i-1)\delta + 1}, x_{(i-1)\delta + 2}, \ldots, x_{i\delta} = 0\\
1, &\text{if at least one of }x_{(i-1)\delta + 1}, x_{(i-1)\delta + 2}, \ldots, x_{i\delta} = 1. 
\end{cases}
$$

Then

$$
p_{01}(\delta) = \frac{\sum_{i = 1}^ky_i}{k}
$$

The function `one_zero_sampling()` takes a `simDB` and a $\delta$, and returns the proportion.

```{r}
one_zero_sampling(example_simDB, 3)
one_zero_sampling(example_simDB, 30)
one_zero_sampling(example_simDB, 300)
```

# Simulation protocol

So for each simulation, we have

-   method used - response frequency (RF), or response duration (RD),
-   parameter - 3, 30, 300 interval (RF), or duration (RD).

This gives 6 different types of simulation, which we will repeat 100 times each giving 600 simulations.

Discussion with Eduardo

Frequency

| Percent | Classification |
|---------|----------------|
| 25-50%  | Frequent       |
| 10-15%  | Moderate       |
| 1-2%    | Infrequent     |

: Classification of frequency

# Sampling protocol

For each simulation, we have

-   sampling method - pinpoint - intervals 3, 30, and 300, while for one-zero, we sample every 10 minutes for duration of 3, 30, 300 seconds.

So again we have 6 sampling procedures, that will give use altogether 3600 measurements.

# Results

For each combination of simulation method, simulation parameters, sample method, and sample parameters, we have 100 simulations.  We have calculated the error rate (true proportion of time - estimated proportion of time events occur). These are given in the table below. The 95% intervals are based on percentile intervals. 


## Summary statistics

```{r, echo = FALSE}
results_tab <- tar_read(results_tab) %>% 
  rename(
    `Simulation method` = methods,
    `Simulation parameters` = params,
    `Proportion of time event occurs` = true_p,
    `Sampling method` = method,
    `Sampling parameters` = delta,
    `Mean error` = m,
    `Lower 95% percentile of error` = lwr,
    `Upper 95% percentile of error` = upr
    ) %>%  
  group_by(`Simulation method`, `Sampling method`) %>% gt() %>% 
  tab_header(
    title = "Summary statistics of error rate for each simulation type", 
    subtitle = "Grouped by simulation method and sampling method"
    ) %>% 
  tab_source_note(
    source_note = "RD: Response duration"
  ) %>% 
  tab_source_note(
    source_note = "RF: Response frequency"
  ) %>% 
  tab_source_note(
    source_note = "PP: Pinpoint sampling"
  ) %>% 
  tab_source_note(
    source_note = "01: One-zero sampling"
  ) 
results_tab
```

Notes, easiest way to get table out, is to save as RTF, and then paste into word. 

```{r}
results_tab %>% 
  gtsave("tabs/results.rtf")
```


## Response frequency simulation

```{r, echo = FALSE}
tar_read(results_plot_RF) + 
  labs(title = "95% percentile intervals of error rates for the response frequency simulation method")
```

```{r}
ggsave("figs/RF_plot.jpg", width = 10, height = 6)
```

Notice that the zero-one sampling is biased in nearly all cases, over-estimating the true percent of the time that the event occurs. This overestimation gets worse as the sampling parameter increases. 

With pinpoint sampling, we find that the method is unbiased, but the variability of the estimate increases as the interval between samples increases. 

## Response duration simulation

```{r, echo = FALSE}
tar_read(results_plot_RD) +
  labs(title = "95% percentile intervals of error rates for the response duration simulation method")
```

```{r}
ggsave("figs/RD_plot.jpg", width = 10, height = 6)
```

Again we see that one-zero sampling is a poor estimator, not quite as consistently bad as with response frequency, but still pretty bad. Again, we see the over-estimation increases as the duration of the one-zero sampling increases. 

# Test for difference between sampling methods

To do this, we use a Friedman test to deal with the non-normality of the errors. I have treated the simulation and sampling parameter values as blocks. 

## Response duration

```{r}
tar_read(test_results_RD)
```

We see that there is a statistically significant difference between the sampling methods at the 5% level for the response duration method. 

## Response frequency

```{r}
tar_read(test_results_RF)
```

## Post-hoc

```{r}
post_hoc_tab <- tar_read(post_hoc_tab) %>% 
  select(
    `Simulation method` = methods, 
    `Simulation parameters` = params, 
    `Sampling parameters` = delta, 
    `P-value` = p
  ) %>% 
  group_by(`Simulation method`) %>% 
  pivot_wider(names_from = `Sampling parameters`,values_from = `P-value`) %>% 
  gt() %>% 
  tab_spanner(
    label = "Sampling parameters", 
    columns = vars(`5`, `50`, `500`)
  ) %>% 
  tab_header(
    title = "P-value for comparision of pinpoint sampling to one-zero sampling for each simulation and sampling method", 
    subtitle = "P-values adjusted by FDR method"
    )
post_hoc_tab
```

```{r}
post_hoc_tab %>% 
  gtsave("tabs/post_hoc.rtf")
```
